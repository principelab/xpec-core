{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71ff4d0",
   "metadata": {},
   "source": [
    "## Within-Subject Classification Pipeline\n",
    "#### Using top networks per epoch to classify tasks (S1-S7) within a subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a38fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multipec.simulation_utils import direct\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e080ef2",
   "metadata": {},
   "source": [
    "Paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdee0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repo_root(marker=\"setup.py\"):\n",
    "    path = Path.cwd()\n",
    "    while not (path / marker).exists() and path != path.parent:\n",
    "        path = path.parent\n",
    "    return path\n",
    "\n",
    "project_root = find_repo_root()\n",
    "\n",
    "data_folder = project_root/\"data/output/eeg/\"\n",
    "results_folder = project_root/\"data/results/eeg/\"\n",
    "figures_folder = project_root/\"data/figures/eeg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f17f76",
   "metadata": {},
   "source": [
    "Helper functions: Load top networks for a subject. Also, make sure to select only the networks with seeds within the lowest percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ddd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_percentile_pairs(pairs_dict, nodes, percentile):\n",
    "    \"\"\"\n",
    "    Convert PEC_pairs output into real node pairs\n",
    "    and select lowest percentile.\n",
    "    \"\"\"\n",
    "    directed_pairs = direct(pairs_dict, nodes)\n",
    "\n",
    "    if len(directed_pairs) == 0: return set()\n",
    "    \n",
    "    sorted_pairs = sorted(directed_pairs.items(), key=lambda x: x[1])\n",
    "\n",
    "    k = max(1, int(len(sorted_pairs) * percentile / 100.0))\n",
    "\n",
    "    selected = sorted_pairs[:k]\n",
    "\n",
    "    return set(tuple(sorted(pair)) for pair, _ in selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_top_nets(subject_id, data_folder, verbose=True, percentile=1.0):\n",
    "    \"\"\"\n",
    "    Load networks for a subject, but KEEP ONLY networks whose seed pair\n",
    "    belongs to the lowest PEC percentile in that epoch.\n",
    "\n",
    "    Adaptive:\n",
    "    start at 1% and relax by 0.5% until >= top_k networks exist.\n",
    "    UNLESS the epoch naturally has <10 networks (then keep all).\n",
    "    \"\"\"\n",
    "    files = glob.glob(os.path.join(data_folder, f\"nets_*_S*_{subject_id}_epoch*.p\"))\n",
    "    all_networks = []\n",
    "\n",
    "    for file in sorted(files):\n",
    "\n",
    "        basename = os.path.basename(file)\n",
    "\n",
    "        match = re.search(r'nets_(.+?)_(S\\d+)_(\\d+)_epoch(\\d+)\\.p$', basename)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        label, task, _, epoch = match.groups()\n",
    "        epoch = int(epoch)\n",
    "        subject = f\"sub{subject_id}\"\n",
    "\n",
    "        #  load networks \n",
    "        with open(file, \"rb\") as f:\n",
    "            nets_list = pickle.load(f)\n",
    "\n",
    "        n_nets = len(nets_list)\n",
    "        if verbose:\n",
    "            print(f\"{subject} {task} epoch {epoch}: {n_nets} networks in file\")\n",
    "        \n",
    "        # CASE 1: too few networks\n",
    "        if n_nets < 10:\n",
    "            if verbose:\n",
    "                print(\"   <10 networks → skipping pair filtering\")\n",
    "            for entry in nets_list:\n",
    "                nodes_tuple, pec_values = entry\n",
    "                all_networks.append({\n",
    "                    'sigma': label,\n",
    "                    'task': task,\n",
    "                    'subject': subject,\n",
    "                    'epoch': epoch,\n",
    "                    'net_nodes': nodes_tuple,\n",
    "                    'pec_progress': pec_values,\n",
    "                    'pec': pec_values[-1] if pec_values else None,\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        # CASE 2 — enough networks: filter by pairs\n",
    "        pairs_file = os.path.join(data_folder, f\"pairs_{task}_{subject_id}_epoch{epoch}.p\")\n",
    "\n",
    "        if not os.path.exists(pairs_file):\n",
    "            if verbose:\n",
    "                print(\"   pairs file missing → keeping all networks\")\n",
    "            seed_pairs = None\n",
    "        else:\n",
    "            with open(pairs_file, \"rb\") as f:\n",
    "                pairs = pickle.load(f)\n",
    "\n",
    "            nodes = list(range(62))\n",
    "            selected_networks = []\n",
    "\n",
    "            while percentile <= 10.0:\n",
    "\n",
    "                seed_pairs = lowest_percentile_pairs(pairs, nodes, percentile)\n",
    "                filtered = []\n",
    "                for entry in nets_list:\n",
    "                    nodes_tuple, pec_values = entry\n",
    "\n",
    "                    if len(nodes_tuple) < 2:\n",
    "                        continue\n",
    "                    seed = tuple(sorted(nodes_tuple[:2]))\n",
    "\n",
    "                    if seed in seed_pairs:\n",
    "                        filtered.append(entry)\n",
    "\n",
    "                if len(filtered) >= 10:\n",
    "                    selected_networks = filtered\n",
    "                    break\n",
    "                percentile += 0.5\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"   selected {len(selected_networks)} nets using {percentile:.1f}% pairs\")\n",
    "            nets_list = selected_networks if selected_networks else nets_list\n",
    "\n",
    "        # store\n",
    "        for entry in nets_list:\n",
    "            nodes_tuple, pec_values = entry\n",
    "            all_networks.append({\n",
    "                'sigma': label,\n",
    "                'task': task,\n",
    "                'subject': subject,\n",
    "                'epoch': epoch,\n",
    "                'net_nodes': nodes_tuple,\n",
    "                'pec_progress': pec_values,\n",
    "                'pec': pec_values[-1] if pec_values else None,\n",
    "            })\n",
    "    return all_networks\n",
    "\n",
    "def select_top_networks(networks, top_k=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Select top_k networks (lowest PEC) per subject per task per epoch.\n",
    "    Also prints how many networks were available vs kept.\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(list)\n",
    "    \n",
    "    for net in networks:\n",
    "        key = (net['subject'], net['task'], net['epoch'])\n",
    "        grouped[key].append(net)\n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nTOP-K SELECTION \")\n",
    "\n",
    "    for key, nets in sorted(grouped.items()):\n",
    "        subject, task, epoch = key\n",
    "        # remove None PEC (rare but dangerous)\n",
    "        nets = [n for n in nets if n['pec'] is not None]\n",
    "        # sort ascending (best PEC first)\n",
    "        sorted_nets = sorted(nets, key=lambda x: x['pec'])\n",
    "        best_nets = sorted_nets[:top_k]\n",
    "        filtered.extend(best_nets)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"{task} | epoch {epoch}: \"\n",
    "                f\"{len(nets)} available -> {len(best_nets)} selected\"\n",
    "            )\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff384330",
   "metadata": {},
   "source": [
    "Node activation map for each epoch, to obtain a single spatial functional brain pattern for brain state classification (rather than network classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_epoch_activation(networks_epoch, n_channels=62):\n",
    "    \"\"\"\n",
    "    Convert all networks of ONE epoch into a node activation map.\n",
    "    \"\"\"\n",
    "    activation = np.zeros(n_channels)\n",
    "\n",
    "    for net in networks_epoch:\n",
    "        nodes = net['net_nodes']\n",
    "        w = 1.0 / (net['pec'] + 1e-6)   # IMPORTANT: smaller PEC = stronger network\n",
    "\n",
    "        for n in nodes:\n",
    "            activation[n] += w\n",
    "\n",
    "    # z-score normalization (subject independent)\n",
    "    if np.std(activation) > 0:\n",
    "        activation = (activation - np.mean(activation)) / np.std(activation)\n",
    "\n",
    "    return activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee11701",
   "metadata": {},
   "source": [
    "ROI features. We extract cognitive meaning by compuoting energy inside cortical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9dcc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROIS = {\n",
    "    \"visual\": [27,28,29,30,33,34,35,36],\n",
    "    \"temporal_L\": [16,17,21,22,37,42,45,47,49,51],\n",
    "    \"temporal_R\": [18,19,23,24,38,43,46,48,50,53],\n",
    "    \"central\": [11,12,13,14,17,18,19,20],\n",
    "    \"parietal\": [21,22,23,24,25,26],\n",
    "    \"frontal\": [5,6,7,8,9,54,55,56,57],\n",
    "    \"prefrontal\": [0,1,2,3,4,58,59,60,61]\n",
    "}\n",
    "\n",
    "def compute_roi_features(activation):\n",
    "    feats = []\n",
    "    for roi_nodes in ROIS.values():\n",
    "        feats.append(np.mean(activation[roi_nodes]))\n",
    "    return np.array(feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eda5dd",
   "metadata": {},
   "source": [
    "Spatial centroid of brian state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27515fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_centroid(activation, coords):\n",
    "    coords_array = np.array([coords[i] for i in range(62)])\n",
    "    weights = np.abs(activation) + 1e-8\n",
    "    centroid = np.average(coords_array, axis=0, weights=weights)\n",
    "    return centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14975465",
   "metadata": {},
   "source": [
    "Hemispheric dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hemispheric_balance(activation, channel_names):\n",
    "    left = [i for i,ch in enumerate(channel_names) if ch.endswith(('1','3','5','7'))]\n",
    "    right = [i for i,ch in enumerate(channel_names) if ch.endswith(('2','4','6','8'))]\n",
    "    L = np.mean(np.abs(activation[left]))\n",
    "    R = np.mean(np.abs(activation[right]))\n",
    "    return L - R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac0d54",
   "metadata": {},
   "source": [
    "64-channel 10–10 system coordinates (x, y, z on unit sphere) from https://github.com/sccn/eeglab/blob/master/sample_locs/GSN64v2_0.sfp. A1 (31) and A2 (32) electrodes (references) were removed during preprocessing, so there is 62 total electrodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_64 = {\n",
    "    0: (4.82147, 8.46376, -0.0639843),\n",
    "    1: (3.44999, 9.06441, 2.97064),\n",
    "    2: (1.90275, 7.64198, 6.40285),\n",
    "    3: (0.0, 5.21914, 8.19540),\n",
    "    4: (-1.72628, 2.48578, 8.55637),\n",
    "    5: (1.59679, 10.5184, 0.515771),\n",
    "    6: (0.0, 9.90900, 4.03918),\n",
    "    7: (-1.90275, 7.64198, 6.40285),\n",
    "    8: (-3.44910, 4.88753, 7.22782),\n",
    "    9: (0.0, 10.2056, -1.61817),\n",
    "    10: (-1.59679, 10.5184, 0.515771),\n",
    "    11: (-3.44999, 9.06441, 2.97064),\n",
    "    12: (-4.77336, 6.69969, 4.57236),\n",
    "    13: (-4.82147, 8.46376, -0.0639843),\n",
    "    14: (-6.08393, 6.13847, 1.57273),\n",
    "    15: (-6.30320, 3.70148, 4.22719),\n",
    "    16: (-5.28740, 1.30272, 6.65006),\n",
    "    17: (-2.89239, -1.49571, 8.18118),\n",
    "    18: (-6.49644, 4.76669, -1.91820),\n",
    "    19: (-7.25734, 2.59912, 0.903444),\n",
    "    20: (-7.34336, 0.287481, 3.19674),\n",
    "    21: (-5.94121, -2.42769, 5.97948),\n",
    "    22: (-6.70321, 2.34743, -5.19168),\n",
    "    23: (-7.64626, -1.18501, 0.271229),\n",
    "    24: (-7.24521, -2.89502, 2.68804),\n",
    "    25: (-7.06353, -3.24230, -2.70838),\n",
    "    26: (-6.78539, -4.61900, -0.234471),\n",
    "    27: (-5.98166, -5.75781, 2.97462),\n",
    "    28: (-3.38310, -5.87378, 6.25655),\n",
    "    29: (0.0, -4.05271, 8.13937),\n",
    "    30: (-5.64139, -6.07135, -3.51430),\n",
    "    31: (-4.88083, -7.67925, -0.352208),\n",
    "    32: (-3.16769, -8.12261, 3.13635),\n",
    "    33: (0.0, -7.14834, 5.87509),\n",
    "    34: (-4.27348, -6.69422, -6.35512),\n",
    "    35: (-3.08195, -8.59131, -3.53248),\n",
    "    36: (-1.79973, -9.42935, -0.256676),\n",
    "    37: (0.0, -8.99684, 2.56482),\n",
    "    38: (0.0, -9.12413, -3.87835),\n",
    "    39: (1.79973, -9.42935, -0.256676),\n",
    "    40: (3.16769, -8.12261, 3.13635),\n",
    "    41: (3.38310, -5.87378, 6.25655),\n",
    "    42: (2.89239, -1.49571, 8.18118),\n",
    "    43: (3.08195, -8.59131, -3.53248),\n",
    "    44: (4.88083, -7.67925, -0.352208),\n",
    "    45: (5.98166, -5.75781, 2.97462),\n",
    "    46: (5.94121, -2.42769, 5.97948),\n",
    "    47: (5.64139, -6.07135, -3.51430),\n",
    "    48: (6.78539, -4.61900, -0.234471),\n",
    "    49: (7.24521, -2.89502, 2.68804),\n",
    "    50: (7.06353, -3.24230, -2.70838),\n",
    "    51: (7.64626, -1.18501, 0.271229),\n",
    "    52: (7.34336, 0.287481, 3.19674),\n",
    "    53: (5.28740, 1.30272, 6.65006),\n",
    "    54: (1.72628, 2.48578, 8.55637),\n",
    "    55: (7.25734, 2.59912, 0.903444),\n",
    "    56: (6.30320, 3.70148, 4.22719),\n",
    "    57: (3.44910, 4.88753, 7.22782),\n",
    "    58: (6.70321, 2.34743, -5.19168),\n",
    "    59: (6.49644, 4.76669, -1.91820),\n",
    "    60: (6.08393, 6.13847, 1.57273),\n",
    "    61: (4.77336, 6.69969, 4.57236),\n",
    "    62: (3.82315, 7.90175, -7.06073),\n",
    "    63: (-3.82315, 7.90175, -7.06073)\n",
    "}\n",
    "\n",
    "def exclude_and_reindex_channels(coords_dict, exclude_indices=[31, 32]):\n",
    "    \"\"\"\n",
    "    Remove specific channels and reindex the remaining channels.\n",
    "    \n",
    "    Parameters:\n",
    "    - coords_dict (dict): Original channel coordinates {idx: (x, y, z)}\n",
    "    - exclude_indices (list): List of indices to remove\n",
    "    \n",
    "    Returns:\n",
    "    - new_coords (dict): Reindexed channel coordinates\n",
    "    \"\"\"\n",
    "    # Remove excluded channels\n",
    "    filtered_items = [(idx, coord) for idx, coord in coords_dict.items() if idx not in exclude_indices]\n",
    "    \n",
    "    # Reindex from 0\n",
    "    new_coords = {new_idx: coord for new_idx, (_, coord) in enumerate(filtered_items)}\n",
    "    \n",
    "    return new_coords\n",
    "\n",
    "coords_62 = exclude_and_reindex_channels(coords_64, exclude_indices=[31, 32])\n",
    "\n",
    "# Hemisphere map\n",
    "hemisphere_62 = {i: 'L' if i % 2 == 0 else 'R' for i in range(62)}  # simplification\n",
    "\n",
    "channel_names = [\n",
    "    \"Fp1\", \"Fp2\", \"F7\", \"F3\", \"Fz\", \"F4\", \"F8\", \"FC5\", \"FC1\", \"FC2\", \"FC6\", \"T7\",\n",
    "    \"C3\", \"C4\", \"T8\", \"TP9\", \"CP5\", \"CP1\", \"CP2\", \"CP6\", \"TP10\", \"P7\", \"P3\", \"Pz\",\n",
    "    \"P4\", \"P8\", \"O1\", \"Oz\", \"O2\", \"Iz\", \"AF7\", \"AF3\", \"AFz\", \"AF4\",\n",
    "    \"AF8\", \"F5\", \"F1\", \"F2\", \"F6\", \"FT7\", \"FC3\", \"FCz\", \"FC4\", \"FT8\", \"C5\", \"C1\",\n",
    "    \"C2\", \"C6\", \"TP7\", \"CP3\", \"CPz\", \"CP4\", \"TP8\", \"P5\", \"P1\", \"P2\", \"P6\", \"PO7\",\n",
    "    \"PO3\", \"POz\", \"PO4\", \"PO8\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f007ff",
   "metadata": {},
   "source": [
    "Function for building feature matrix X and labels y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2176b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_epoch_dataset(subject_networks, coords, hemisphere_map):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    groups = []\n",
    "\n",
    "    # group networks by epoch+task\n",
    "    epoch_groups = defaultdict(list)\n",
    "\n",
    "    for net in subject_networks:\n",
    "        key = (net['task'], net['epoch'])\n",
    "        epoch_groups[key].append(net)\n",
    "\n",
    "    for (task, epoch), nets in epoch_groups.items():\n",
    "\n",
    "        activation = build_epoch_activation(nets)\n",
    "\n",
    "        roi_feats = compute_roi_features(activation)\n",
    "        centroid = activation_centroid(activation, coords)\n",
    "        hemi = hemispheric_balance(activation, hemisphere_map)\n",
    "\n",
    "        feature_vector = np.concatenate([\n",
    "            activation,      # 62\n",
    "            roi_feats,       # 7\n",
    "            centroid,        # 3\n",
    "            [hemi]           # 1\n",
    "        ])\n",
    "\n",
    "        X.append(feature_vector)\n",
    "        y.append(task)\n",
    "        groups.append(epoch)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    groups = np.array(groups)\n",
    "\n",
    "    # standardize\n",
    "    # comment out if you want to keep original activation values (for visualization or non-linear models)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    return X, y, groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed7418",
   "metadata": {},
   "source": [
    "Leave-One-Epoch-Out (LOEO) classification function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ee19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_epochs(X, y, groups):\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=0.95)), # retain 95% variance\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=3000,\n",
    "            class_weight='balanced',\n",
    "            solver='lbfgs'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    all_prob = []\n",
    "\n",
    "    for train_idx, test_idx in logo.split(X, y, groups):\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # IMPORTANT: pipeline is fit ONLY on training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        pred = model.predict(X_test)\n",
    "        prob = model.predict_proba(X_test)\n",
    "\n",
    "        all_true.extend(y_test)\n",
    "        all_pred.extend(pred)\n",
    "        all_prob.extend(prob)\n",
    "\n",
    "    print(classification_report(all_true, all_pred))\n",
    "\n",
    "    Y_bin = label_binarize(all_true, classes=np.unique(y))\n",
    "    auc = roc_auc_score(Y_bin, np.array(all_prob), multi_class='ovr')\n",
    "    print(\"Overall ROC-AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8143378",
   "metadata": {},
   "source": [
    "Load subject data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ee419",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = '01'\n",
    "tasks = [f\"S{i}\" for i in range(1, 8)]  # S1-S7\n",
    "epochs = list(range(10))  # 0-9\n",
    "\n",
    "# 1) Load and select networks\n",
    "subject_networks = select_top_networks(\n",
    "    load_subject_top_nets(subject_id, data_folder, percentile=1.0),\n",
    "    top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df818b32",
   "metadata": {},
   "source": [
    "Build classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5606571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build dataset at EPOCH LEVEL\n",
    "X, y, groups = build_epoch_dataset(\n",
    "    subject_networks,\n",
    "    coords=coords_62,\n",
    "    hemisphere_map=channel_names\n",
    ")\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Samples per task:\")\n",
    "from collections import Counter\n",
    "print(Counter(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206383b",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Classify\n",
    "classify_epochs(X, y, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02270e57",
   "metadata": {},
   "source": [
    "### Visualization and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8e071",
   "metadata": {},
   "source": [
    "Plotting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22331720",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (\n",
    "    [f\"activation_{i}\" for i in range(62)] +\n",
    "    [f\"roi: {lobe}\" for lobe in ROIS.keys()] +\n",
    "    [\"centroid_x\", \"centroid_y\", \"centroid_z\"] +\n",
    "    [\"hemispheric_balance\"]\n",
    ")\n",
    "\n",
    "TASK_ORDER = [\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\"S7\"]\n",
    "\n",
    "TASK_COLORS = {\n",
    "    \"S1\": \"#08306b\",   # dark blue\n",
    "    \"S2\": \"#2171b5\",   # medium blue\n",
    "    \"S3\": \"#6baed6\",   # light blue\n",
    "\n",
    "    \"S4\": \"#7f2704\",   # dark orange\n",
    "    \"S5\": \"#d94801\",   # medium orange\n",
    "    \"S6\": \"#fdae6b\",   # light orange\n",
    "\n",
    "    \"S7\": \"#6b6b6b\"    # gray\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281c813",
   "metadata": {},
   "source": [
    "Build feature dataframe. X and y need to be non-standardized (comment out `StandardScaler` from `build_epoch_dataset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5b828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "roi: prefrontal    0.161889\n",
       "roi: temporal_R    0.171981\n",
       "roi: visual        0.193674\n",
       "roi: temporal_L    0.197799\n",
       "roi: frontal       0.202298\n",
       "                     ...   \n",
       "activation_43      1.306552\n",
       "activation_6       1.314970\n",
       "activation_48      1.373625\n",
       "activation_10      1.499552\n",
       "activation_16      1.546583\n",
       "Length: 73, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_feature_dataframe(X, y, groups, feature_names):\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df[\"task\"] = y\n",
    "    df[\"epoch\"] = groups\n",
    "    return df\n",
    "\n",
    "\n",
    "df = build_feature_dataframe(X, y, groups, feature_names)\n",
    "\n",
    "df[feature_names].std().sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05601c78",
   "metadata": {},
   "source": [
    "Plot a specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(df, feature):\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    tasks = [t for t in TASK_ORDER if t in df[\"task\"].unique()]\n",
    "\n",
    "    for task in tasks:\n",
    "        sub = df[df[\"task\"] == task]\n",
    "        sub = sub.sort_values(\"epoch\")\n",
    "\n",
    "        plt.plot(\n",
    "            sub[\"epoch\"],\n",
    "            sub[feature],\n",
    "            marker=\"o\",\n",
    "            linewidth=1.8,\n",
    "            markersize=4,\n",
    "            color=TASK_COLORS.get(task, \"black\"),\n",
    "            label=task\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(feature)\n",
    "    plt.title(feature)\n",
    "    plt.legend(ncol=4, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_feature(df, \"hemispheric_balance\")\n",
    "# plot_feature(df, \"centroid_x\")\n",
    "# plot_feature(df, \"roi_3\")\n",
    "# plot_feature(df, \"activation_12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a7466",
   "metadata": {},
   "source": [
    "Plot the distribution of a specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(df, feature):\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    tasks = [t for t in TASK_ORDER if t in df[\"task\"].unique()]\n",
    "\n",
    "    for task in tasks:\n",
    "        vals = df[df[\"task\"] == task][feature]\n",
    "\n",
    "        plt.hist(\n",
    "            vals,\n",
    "            bins=20,\n",
    "            alpha=0.5,\n",
    "            color=TASK_COLORS.get(task, \"black\"),\n",
    "            label=task,\n",
    "            density=True\n",
    "        )\n",
    "\n",
    "    plt.title(feature)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_feature_distribution(df, \"hemispheric_balance\")\n",
    "# plot_feature_distribution(df, \"centroid_x\")\n",
    "# plot_feature_distribution(df, \"roi_3\")\n",
    "# plot_feature_distribution(df, \"activation_12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f3ffd",
   "metadata": {},
   "source": [
    "Plot all or groups of features at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_feats = [f for f in feature_names if f.startswith(\"activation\")]\n",
    "roi_feats = [f for f in feature_names if f.startswith(\"roi\")]\n",
    "global_feats = [\"centroid_x\", \"centroid_y\", \"centroid_z\", \"hemispheric_balance\"]\n",
    "\n",
    "def plot_all_features(df, feature_names):\n",
    "    for feat in feature_names:\n",
    "        plot_feature(df, feat)\n",
    "\n",
    "plot_all_features(df, activation_feats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
